You are Claude, an AI assistant tasked with fixing issues in a GitHub repository.

Issue #409: [claude-fix] Issue #407: System Stabilization - Token Costs, Uncertainty Templates, Logging & Document Response

Issue Description:
# Issue #407: System Stabilization - Token Costs, Uncertainty Templates, Logging & Document Responses

## Priority: CRITICAL
## Affects: Cost sustainability, user experience, system reliability

---

## Executive Summary

The system has multiple interconnected issues causing:
1. **Token cost explosion** - 92K tokens / $0.33 for 7 calls (unsustainable)
2. **Empty uncertainty templates** - Template structure printed instead of filled content
3. **Railway rate limiting** - 3,061+ messages dropped due to excessive logging
4. **Document response mismatch** - Uploaded documents not being properly addressed

These must be fixed together because they share root causes in the response generation and context loading pipeline.

---

## FOUNDATIONAL ALIGNMENT

Per the Bible documents, this system operates on:

**Truth > Helpfulness > Engagement**

All fixes must maintain:
- Token efficiency is not optional - it's foundational architecture
- Uncertainty handling follows the 4-part structure (not printed templates)
- Anti-engagement: Complete answers, decisive endings
- Cost control enables the system to exist at all

---

## ISSUE 1: Token Cost Explosion

### Current State
```
Session: 7 calls, 92,077 tokens, $0.3268
Large document upload: 182,008 characters (~45K tokens)
```

### Root Cause
Documents are processed at FULL SIZE before any compression. The intelligent storage compression (38:1 ratio) works for STORAGE but not for the initial AI call.

### Required Fix (Per Technical Standards)

**File: `api/chat.js` or document processing handler**

Add pre-processing BEFORE sending to AI:

```javascript
// COST CONTROL - Per Technical Standards
const TOKEN_BUDGETS = {
  simple: 10000,    // Basic questions
  medium: 30000,    // Context-needed queries  
  complex: 80000,   // Full reasoning
  maximum: 110000   // Hard limit
};

const SESSION_LIMITS = {
  maxUploadedTokens: 10000,      // Per Bible: document limit
  maxMemoryTokens: 2500,         // Per Bible: extraction limit
  maxConversationTokens: 20000,  // History limit
  totalSessionLimit: 35000       // Absolute max
};

function preprocessDocument(content, queryType = 'medium') {
  const budget = TOKEN_BUDGETS[queryType];
  const charLimit = budget * 4; // ~4 chars per token
  
  if (content.length <= charLimit) {
    return { processed: content, truncated: false };
  }
  
  // Extract key content within budget
  const summary = extractKeyContent(content, charLimit);
  
  console.log(`[COST-CONTROL] Document truncated: ${content.length} → ${summary.length} chars`);
  
  return {
    processed: summary,
    originalLength: content.length,
    truncated: true,
    truncationNote: `Document summarized from ${Math.round(content.length/1000)}K to ${Math.round(summary.length/1000)}K characters for cost efficiency.`
  };
}

function extractKeyContent(content, maxChars) {
  // Strategy 1: If structured document, extract sections
  const sections = content.split(/\n#{1,3}\s+/);
  if (sections.length > 1) {
    let result = '';
    for (const section of sections) {
      if (result.length + section.length > maxChars) break;
      result += section + '\n\n';
    }
    return result.trim();
  }
  
  // Strategy 2: First N characters with paragraph boundary
  const truncated = content.substring(0, maxChars);
  const lastParagraph = truncated.lastIndexOf('\n\n');
  return lastParagraph > maxChars * 0.8 
    ? truncated.substring(0, lastParagraph)
    : truncated;
}
```

### Integration Point

In the document upload/processing flow:
```javascript
// BEFORE sending to AI
const processedDoc = preprocessDocument(uploadedContent, classifyQuery(userMessage));
if (processedDoc.truncated) {
  // Add note to context so AI knows document was summarized
  context.documentNote = processedDoc.truncationNote;
}
```

---

## ISSUE 2: Empty Uncertainty Templates

### Current State (BROKEN)
```
State what IS known:
_[Based on established patterns and principles...]_

Identify what is unknown:
_[Key factors that would change the answer...]_
```

The AI is outputting the TEMPLATE instead of filling it in.

### Root Cause
The uncertainty framework is being passed as OUTPUT FORMAT rather than INSTRUCTIONS. The system prompt likely contains the template structure with placeholders, and the AI is treating it as literal text to output.

### Required Fix (Per Master Specification)

**Find and modify the system prompt section that contains uncertainty handling.**

WRONG (current - template as output):
```javascript
const uncertaintyFormat = `
When uncertain, use this format:

State what IS known:
_[Based on established patterns and principles...]_

Identify what is unknown:
_[Key factors that would change the answer...]_
`;
```

CORRECT (instructions, not template):
```javascript
const uncertaintyInstructions = `
UNCERTAINTY HANDLING - MANDATORY PATTERN

When you lack sufficient information to give a definitive answer, you MUST:

1. HONEST ADMISSION (required first)
   Say directly: "I don't have enough information about [specific aspect] to give you a definitive answer, and being honest with you matters more than appearing knowledgeable."

2. EXPLAIN WHY UNCERTAIN (required second)
   State clearly:
   - What I know: [List actual facts you have]
   - What I don't know: [List specific gaps]
   - Why this matters: [Explain impact of not knowing]

3. PROVIDE ALTERNATIVES (required third)
   Offer comparable scenarios:
   - "If your situation is like [Scenario A]: [specific guidance] (Confidence: 0.X)"
   - "If your situation is like [Scenario B]: [alternative path] (Confidence: 0.X)"

4. EMPOWER USER (required fourth)
   State what would help: "To give you a definitive answer, I would need [specific information]. Or you could [alternative action]."

CRITICAL: Fill in ALL brackets with actual content. Never output placeholder text or template markers. Each section must contain real, specific information based on the query.
`;
```

### Verification Test
After fix, ask: "Should I expand my business to a second location?"

Expected response should have FILLED content like:
```
I don't have enough information about your specific business performance, market conditions, and financial position to tell you whether a second location makes sense. Being honest with you matters more than giving confident-sounding advice.

What I know: Second locations can accelerate growth when the first location is stable and profitable.

What I don't know:
- Is your first location consistently profitable?
- Do you have systems that can scale?
- What's your capital situation for a slow ramp-up period?

If you're like businesses with 2+ years profitability and documented systems: Expansion can work well. (Confidence: 0.70)

If you're like businesses still optimizing their first location: The risk typically outweighs the opportunity. (Confidence: 0.65)

To give you a definitive answer, I would need: your P&L for the past 12 months, your current cash reserves, and whether you have manager-level staff who could run a second location.
```

---

## ISSUE 3: Railway Rate Limiting / Excessive Logging

### Current State
```
Railway rate limit of 500 logs/sec reached for replica...
Messages dropped: 3061
```

### Root Cause
The system is logging too verbosely - every memory operation, every embedding generation, every routing decision at full detail.

### Required Fix

**File: Wherever logging is configured (likely `lib/logger.js` or scattered console.log statements)**

Create tiered logging:
```javascript
// LOGGING CONFIGURATION
const LOG_LEVEL = process.env.LOG_LEVEL || 'info'; // debug, info, warn, error

const LOG_LEVELS = {
  debug: 0,
  info: 1,
  warn: 2,
  error: 3
};

function log(level, category, message, data = null) {
  if (LOG_LEVELS[level] < LOG_LEVELS[LOG_LEVEL]) return;
  
  const timestamp = new Date().toISOString();
  const prefix = `[${timestamp}] [${category}]`;
  
  // PRODUCTION: Summary only
  if (LOG_LEVEL === 'info' || LOG_LEVEL === 'warn') {
    switch(category) {
      case 'MEMORY':
        // Only log summary, not full content
        console.log(`${prefix} ${message}`);
        break;
      case 'ROUTING':
        // Only log decision, not full analysis
        console.log(`${prefix} ${message}`);
        break;
      case 'COST':
        // Always log cost info
        console.log(`${prefix} ${message}`, data);
        break;
      default:
        console.log(`${prefix} ${message}`);
    }
  } else if (LOG_LEVEL === 'debug') {
    // Full logging for debugging
    console.log(`${prefix} ${message}`, data || '');
  }
}

// USAGE REPLACEMENTS:

// BEFORE (verbose)
console.log(`[INTELLIGENT-STORAGE] Full memory content: ${JSON.stringify(memory)}`);
console.log(`[DEDUP] Analyzing similarity with existing memories...`);
console.log(`[DEDUP] Token comparison: ${tokens.join(', ')}`);

// AFTER (summary)
log('info', 'MEMORY', `Stored: ID=${memory.id}, ${memory.tokens} tokens`);
log('debug', 'DEDUP', 'Token analysis', { count: tokens.length });
```

### Environment Variable
Add to Railway:
```
LOG_LEVEL=info  # Production
LOG_LEVEL=debug # Only when debugging
```

---

## ISSUE 4: Document Response Mismatch

### Current State
```
[RESPONSE-CONTRACT] Response does not address document content
```

Logs show: User uploads document → AI responds about something else entirely (like the Bill of Sale example where it talked about a previous document instead of the current one).

### Root Cause
The document context is either:
1. Not being properly injected into the AI context
2. Being overridden by memory context
3. The AI is confused between "previous document" and "current document"

### Required Fix

**File: `api/chat.js` or wherever document context is built**

```javascript
// DOCUMENT CONTEXT INJECTION - Must be CLEAR and PRIORITIZED
function buildDocumentContext(uploadedDocs, memoryContext) {
  if (!uploadedDocs || uploadedDocs.length === 0) {
    return { documentContext: null, hasCurrentDocument: false };
  }
  
  // Current document takes priority
  const currentDoc = uploadedDocs[uploadedDocs.length - 1]; // Most recent
  
  return {
    documentContext: `
=== CURRENT DOCUMENT (uploaded just now) ===
Filename: ${currentDoc.filename}
Type: ${currentDoc.type}
Content:
${currentDoc.content}
=== END CURRENT DOCUMENT ===

INSTRUCTION: When the user asks about "this document", "the document", "this file", or "what I just uploaded", they are referring to the CURRENT DOCUMENT above. Do NOT reference previous documents from memory unless explicitly asked.
`,
    hasCurrentDocument: true
  };
}

// In the AI prompt construction:
const systemPrompt = `
${baseInstructions}

${documentContext.hasCurrentDocument ? documentContext.documentContext : ''}

${documentContext.hasCurrentDocument ? 
  'PRIORITY: The user has uploaded a document. Address their question about THIS document specifically.' : 
  ''}
`;
```

### Verification Test
1. Upload a PDF
2. Ask "What is this document about?"
3. Response should describe the CURRENT uploaded document, not anything from memory

---

## IMPLEMENTATION ORDER

1. **Token Cost Control** (FIRST - prevents further cost bleed)
   - Add `preprocessDocument()` function
   - Integrate into document upload flow
   - Add `TOKEN_BUDGETS` configuration

2. **Logging Reduction** (SECOND - enables clearer debugging)
   - Add `LOG_LEVEL` environment variable to Railway
   - Replace verbose console.logs with tiered logging
   - Set production to `info` level

3. **Uncertainty Template Fix** (THIRD - user-facing quality)
   - Find system prompt section with uncertainty handling
   - Change from template-as-output to instructions
   - Test with uncertainty-triggering query

4. **Document Response Fix** (FOURTH - user-facing quality)
   - Add clear document context injection
   - Prioritize current document over memory
   - Add explicit instruction about "this document"

---

## FILES LIKELY AFFECTED

Based on logs and architecture:
- `api/chat.js` - Main orchestrator, context building
- `lib/tokenTracker.js` - Cost tracking
- `api/core/intelligence/intelligence.js` - Routing decisions
- System prompt file (wherever Eli/Roxy instructions live)
- Document processing handler
- Logging configuration

---

## SUCCESS CRITERIA

After implementation:

1. **Token Costs**
   - Simple queries: <10K tokens
   - Document queries: <30K tokens (even with large docs)
   - Session average: <$0.10 per conversation

2. **Uncertainty Responses**
   - Template sections filled with actual content
   - No placeholder text visible to users
   - Confidence levels included

3. **Logging**
   - No Railway rate limit warnings
   - Clear, readable logs at `info` level
   - Full detail available at `debug` level

4. **Document Responses**
   - Current document addressed when asked
   - No confusion with previous documents
   - RESPONSE-CONTRACT validation passes

---

## DOCTRINE REFERENCES

- **Master Specification**: Uncertainty Handling Pattern (4-part structure)
- **Technical Standards**: Token Budget Management, Session Limits
- **Alignment Prompts**: Anti-engagement, decisive endings
- **Engineering Specs**: Error handling with fallbacks
- **runtime-contract-validation.js**: Token overflow recovery

---

## NOTES FOR IMPLEMENTATION

This issue follows the proven pattern from Issue #406:
1. Investigate root causes (done above)
2. Trace data flow to find exact locations
3. Fix with doctrine-aligned solutions
4. Verify against test cases

The fixes are interconnected - token control affects logging volume, logging clarity helps debug other issues, and document context affects what gets counted in token budgets.
Your task is to:
1. Analyze the issue carefully to understand the problem
2. Look through the repository to identify the relevant files that need to be modified
3. Make precise changes to fix the issue
4. Use the Edit tool to modify files directly when needed
5. Be minimal in your changes - only modify what's necessary to fix the issue

After making changes, provide a summary of what you did in this format:

---SUMMARY---
[Your detailed summary of changes, including which files were modified and how]
---END SUMMARY---

Remember:
- Be specific in your changes
- Only modify files that are necessary to fix the issue
- Follow existing code style and conventions
- Make the minimal changes needed to resolve the issue
