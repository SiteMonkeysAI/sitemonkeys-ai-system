Now let me write a comprehensive investigation report as a comment to the issue:

---SUMMARY---

# COMPREHENSIVE INVESTIGATION REPORT - SMDEEP Test Failures

## Executive Summary

I have traced all 5 failing SMDEEP tests through the entire codebase. All failures share a **common root cause pattern**: facts are being **extracted and stored correctly**, but the AI **fails to apply reasoning** during response generation even when the facts are present in context.

This is NOT primarily a retrieval problem or storage problem - it's a **reasoning enforcement problem** at the AI generation layer.

---

## COMPLETE DATA FLOW (All Tests Follow This Path)

### INPUT PROCESSING
**Entry Point**: `server.js:329` - `/api/chat` endpoint
- Receives user message
- Validates userId (strict isolation)
- Passes to orchestrator

### ORCHESTRATOR PROCESSING
**File**: `api/core/orchestrator.js`
**Function**: `processRequest()` starting at line 740

**Key Flow Steps**:
1. **Memory Retrieval** (lines 931-948): Calls `#retrieveMemoryContext()`
2. **Context Building** (line 4081): `#buildContextString()` constructs prompt
3. **AI Generation** (lines 3617-3750): Sends to GPT-4/Claude
4. **Enforcement Chain** (line 164): `#runEnforcementChain()` validates response

### MEMORY RETRIEVAL SUBSYSTEM
**File**: `api/core/orchestrator.js:2213` - `#retrieveMemoryContext()`
**Critical Code Path**:
```
orchestrator.js:2258 ‚Üí retrieveSemanticMemories() 
  ‚Üí api/services/semantic-retrieval.js
```

**Max Memories Hard Cap**: Line 2291 - `MAX_MEMORIES_FINAL = 5`
- Strict token efficiency enforcement
- May cut off relevant memories beyond top 5

### STORAGE/EXTRACTION SUBSYSTEM  
**File**: `api/memory/intelligent-storage.js`
**Function**: `storeWithIntelligence()` starting at line 818

**Critical Steps**:
1. **Extraction** (line 1101): `extractKeyFacts()` - Uses GPT-4o-mini with comprehensive prompt
2. **Fingerprint Detection** (line 103): `detectFingerprintFromFacts()`
3. **Storage** (line 1078): `storeCompressedMemory()`

**Extraction Prompt Rules** (lines 1106-1268):
- Preserves exact identifiers, names, numbers
- Handles temporal patterns: "worked 5 years", "left in 2020"
- Preserves relationships: "daughter Emma"
- Includes searchable synonyms
- **CRITICAL**: Rule 16 explicitly preserves temporal patterns for reasoning

### AI GENERATION
**File**: `api/core/orchestrator.js:3617-3750`
**Functions**: `#callAIWithRouting()` 

**Prompt Construction**:
- Line 3617: `contextString = #buildContextString(context, mode)`
- Line 3628: `systemPrompt = #buildSystemPrompt(...)`
- Line 3689 (Claude): Combines `systemPrompt + externalContext + contextString + message`
- Line 3736 (GPT-4): Combines `externalContext + contextString + message`

**Memory Injection Point**: `orchestrator.js:4217-4242`
```javascript
contextStr += `
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üß† PERSISTENT MEMORY CONTEXT - READ ALL ${memoryCount} ITEMS BEFORE RESPONDING
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚ö†Ô∏è NOTE: You have access to ${memoryCount} memories from previous conversations.
If the user asks about something they've told you before, you should find it below.

${memoryText}
```

---

## TEST-BY-TEST FAILURE ANALYSIS

### **INF1: Role Inference (daughter ‚Üí kindergarten)**

**Test Flow**:
1. User: "My daughter Emma just started kindergarten"
2. Later: "What's my relationship to Emma?"

**Expected**: Infer "daughter" relationship
**Actual**: Correctly infers age ~5-6 but fails to infer role

**Data Flow Trace**:

**EXTRACTION (intelligent-storage.js:1101)**:
- Prompt Rule 16 says: "PRESERVE TEMPORAL PATTERNS EXACTLY"
- Prompt includes relationship preservation examples
- However, looking at line 1195-1268, there's NO explicit rule for preserving family relationship keywords
- **FINDING 1**: Extraction prompt lacks explicit "preserve family relationships" rule
- Extracted fact likely becomes: "Emma (child): started kindergarten" 
- The word **"daughter"** may be stripped during compression

**STORAGE**:
- Fact stored in category (routed by `intelligenceSystem.analyzeAndRoute()`)
- Stored with semantic embedding

**RETRIEVAL** (semantic-retrieval.js):
- Query: "What's my relationship to Emma?"
- Semantic search retrieves memories about Emma
- Memory likely retrieved successfully (test shows age inference works)

**AI GENERATION**:
- Memory context injected with Emma info
- **FINDING 2**: System prompt (line 3628) may not explicitly instruct AI to **infer** relationships from context
- AI has "Emma started kindergarten" but not explicit "daughter" relationship
- **FINDING 3**: No validator enforces inferencing from implicit facts

**CONFIDENCE LEVEL**: 85% - Relationship keywords lost in extraction, AI not instructed to infer

---

### **INF3: Temporal Reasoning (2020 - 5 years = 2015)**

**Test Flow**:
1. User: "I worked at Amazon for 5 years"
2. User: "I left Amazon in 2020"  
3. Query: "When did I start working at Amazon?"

**Expected**: Calculate 2020 - 5 = 2015
**Actual**: Has all facts but doesn't calculate timeline

**Data Flow Trace**:

**EXTRACTION (intelligent-storage.js:1173-1180)**:
- Rule 16 EXPLICITLY preserves: "worked 5 years" and "left in 2020"
- Prompt line 1173: `*** CRITICAL: PRESERVE TEMPORAL PATTERNS EXACTLY FOR REASONING ***`
- Extraction should produce:
  - Memory 1: "Work: Amazon. Duration: 5 years"
  - Memory 2: "Work: Amazon. Left in: 2020"

**STORAGE (intelligent-storage.js:385-414)**:
- Line 385: `extractTemporalAnchors()` function extracts metadata
- Detects `end_year: 2020` and `duration_years: 5`
- Stored in metadata.anchors field (line 1069-1077)

**RETRIEVAL**:
- Query: "When did I start working at Amazon?"
- Semantic search retrieves both memories
- **FINDING 4**: Anchors are in DB metadata, but are they passed to AI?

**MEMORY CONTEXT INJECTION** (orchestrator.js:4217-4242):
- Memory text formatted: `${sanitizePII(content)}`
- **FINDING 5**: Metadata anchors (end_year, duration_years) are NOT injected into context!
- Line 2364: `sanitizePII(content)` only passes content, not metadata
- AI receives: "Worked at Amazon 5 years. Left in 2020"
- But does NOT receive structured hints: `{end_year: 2020, duration: 5}`

**AI GENERATION**:
- System prompt line 3628 built by `#buildSystemPrompt()`
- **FINDING 6**: No explicit instruction to "perform temporal arithmetic"
- Line 4201: Context includes "naturally apply temporal reasoning" but this is soft guidance
- No hard requirement: "IF you see duration + end date, CALCULATE start date"

**CONFIDENCE LEVEL**: 90% - Facts preserved but AI not enforced to calculate

---

### **NUA1: Ambiguity Detection (Two Alexes)**

**Test Flow**:
1. User: "Alex is my colleague in marketing at Amazon"
2. User: "Alex is my brother who lives in Seattle"
3. Query: "Tell me about Alex"

**Expected**: Recognize ambiguity - TWO different Alexes
**Actual**: Log shows only ONE Alex returned (ID 7507)

**Data Flow Trace**:

**EXTRACTION & STORAGE**:
- First fact: "Alex (colleague): marketing at Amazon"
- Second fact: "Alex (brother): lives in Seattle"
- **FINDING 7**: No entity disambiguation during storage
- Both stored as memories containing "Alex" but not marked as different entities

**RETRIEVAL** (semantic-retrieval.js):
- Query: "Tell me about Alex"
- Semantic search: `retrieveSemanticMemories(pool, message, {...})`
- Hard cap at line 2291: `MAX_MEMORIES_FINAL = 5`
- **FINDING 8**: Test log shows `returned_ids=[7507]` - only 1 memory retrieved!
- Either:
  - Second Alex memory ranked below top 5 and was cut
  - OR semantic similarity too low for second Alex

**WHY ONE ALEX RETURNED**:
- Line 2295: `memoriesToFormat = result.memories.slice(0, MAX_MEMORIES_FINAL);`
- If both Alexes retrieved but ranked #6 and #7, they're cut
- If query "Tell me about Alex" embeddin is more similar to "colleague Alex", brother Alex ranks lower

**AI GENERATION**:
- AI only receives ONE Alex in context
- Cannot detect ambiguity from incomplete data
- **FINDING 9**: Ambiguity detection requires ALL entities retrieved, but cap prevents this

**VALIDATOR** (lib/validators/conflict-detection.js):
- There IS a conflict detection validator
- But line 2403: `memory_objects: memoriesToFormat` - only passes TOP 5
- Validator cannot detect conflict if second Alex wasn't retrieved

**CONFIDENCE LEVEL**: 95% - Retrieval cap cuts off second entity before ambiguity can be detected

---

### **NUA2: Contextual Tension (allergy vs wife's desire)**

**Test Flow**:
1. User: "I'm severely allergic to cats"
2. User: "My wife really wants to adopt a cat"
3. Query: "Should we get a cat?"

**Expected**: Acknowledge tension between allergy (safety-critical) and wife's desire
**Actual**: Retrieves both facts but response doesn't acknowledge tension

**Data Flow Trace**:

**EXTRACTION & STORAGE**:
- Memory 1: "Health: severely allergic to cats" (category: health_wellness)
- Memory 2: "Family: wife wants to adopt a cat" (category: relationships_social)

**SAFETY-CRITICAL BOOST** (semantic-retrieval.js:122-175):
- Line 122: `applySafetyCriticalBoost()` function
- Detects allergy pattern (line 125: `/\b(allerg(y|ic|ies))\b/i`)
- Applies +0.25 similarity boost
- Marks memory with `safety_boosted: true`

**RETRIEVAL**:
- Query: "Should we get a cat?"
- Line 95: `detectSafetyCriticalCategories()` - detects pets domain
- Line 80: Injects `health_wellness` and `relationships_social` categories
- Both memories retrieved and ranked high

**MEMORY INJECTION** (orchestrator.js:2349-2375):
- Line 2349: Detects `safety_boosted` flag
- Line 2370: Adds emphasis: `"‚ö†Ô∏è SAFETY-CRITICAL INFORMATION (health, medical, allergies):"`
- Both allergy AND wife's desire injected with safety emphasis

**AI GENERATION**:
- Context includes BOTH facts
- System prompt line 4201: "naturally apply temporal reasoning, notice ambiguities, **acknowledge tensions**"
- **FINDING 10**: Instruction to "acknowledge tensions" exists but is SOFT
- No validator enforces tension acknowledgment

**VALIDATOR GAP**:
- No validator checks: "When response addresses conflicting facts, did AI acknowledge the tension?"
- Enforcement chain (orchestrator.js:164) has 8 validators
- None validate "contextual tension acknowledgment"

**CONFIDENCE LEVEL**: 80% - Data present, soft instruction exists, but no enforcement

---

### **CMP2: International Name Preservation (Bj√∂rn, Jos√©, Zhang Wei)**

**Test Flow**:
1. User: "My three key contacts are Zhang Wei, Bj√∂rn Lindqvist, and Jos√© Garc√≠a"
2. Query: "Who are my key contacts?"

**Expected**: Response includes all three names with diacritics
**Actual**: Extraction shows names stored correctly, but names don't appear in response

**Data Flow Trace**:

**EXTRACTION (intelligent-storage.js:1106-1268)**:
- Rule 2 (line 1110): "ALWAYS preserve names exactly as written"
- Rule 10 (line 1126): "Include searchable synonyms in parentheses"
- Extracted: "Contacts: Zhang Wei, Bj√∂rn Lindqvist, Jos√© Garc√≠a (key contacts business professional)"

**STORAGE**:
- Stored with correct Unicode characters
- PostgreSQL supports UTF-8, diacritics preserved in DB

**RETRIEVAL**:
- Query: "Who are my key contacts?"
- Semantic search retrieves contact memory
- **FINDING 11**: Test log says "Memory context present: false"!
- This suggests memory was NOT retrieved at all

**WHY MEMORY NOT RETRIEVED**:
- Line 883-906: Early query classification
- Line 900: `skipMemoryForSimpleQuery` logic
- **FINDING 12**: Query "Who are my key contacts?" contains "my" (personal pronoun)
- Line 894: `hasPersonalIntent` should detect this
- Line 910-915: Safety check - does user have memories?
- **FINDING 13**: If early classification incorrectly categorizes as simple query AND user check fails, retrieval skipped

**ALTERNATIVE HYPOTHESIS**:
- If retrieval WAS attempted:
  - Semantic embedding: "Who are my key contacts?" vs "Contacts: Zhang Wei, Bj√∂rn..."
  - May have low similarity if embeddings don't match well
  - Could rank below TOP 5 cutoff

**AI GENERATION**:
- If memory not retrieved ‚Üí AI has no data ‚Üí responds "I don't have that information"
- Test result: "Names don't appear in response" confirms memory wasn't in context

**CONFIDENCE LEVEL**: 85% - Most likely retrieval skipped OR ranked too low

---

## SHARED CODE PATHS & FAILURE RELATIONSHIPS

### Primary Shared Path (All 5 Tests):
```
server.js:329 /api/chat
  ‚Üì
orchestrator.js:740 processRequest()
  ‚Üì
orchestrator.js:931 #retrieveMemoryContext()
  ‚Üì
semantic-retrieval.js retrieveSemanticMemories()
  ‚Üì
orchestrator.js:2291 MAX_MEMORIES_FINAL cap (5 memories)
  ‚Üì
orchestrator.js:4217 Memory injection into context
  ‚Üì
orchestrator.js:3617 #callAIWithRouting()
  ‚Üì
orchestrator.js:164 #runEnforcementChain()
```

### Shared Failure Points:

**1. HARD CAP (Affects NUA1, possibly CMP2)**
- `orchestrator.js:2291` - `MAX_MEMORIES_FINAL = 5`
- Comment says: "ambiguity detection uses secondary DB query pass via validator"
- But validator relies on `memory_objects` which are ALREADY capped
- **Impact**: Ambiguity/conflicts require multiple entities - cap prevents this

**2. METADATA LOSS (Affects INF3)**
- `orchestrator.js:2364` - `sanitizePII(content)` 
- Only content injected, not metadata (temporal anchors, ordinals, etc.)
- **Impact**: AI doesn't get structured hints for calculations

**3. SOFT REASONING INSTRUCTIONS (Affects INF1, INF3, NUA2)**
- `orchestrator.js:4201` - "naturally apply temporal reasoning, notice ambiguities, acknowledge tensions"
- Soft guidance, no hard requirements
- **Impact**: AI may ignore if not salient enough

**4. NO REASONING VALIDATORS (Affects INF1, INF3, NUA2)**
- Enforcement chain has 8 validators (orchestrator.js:164-399)
- None check: "Did AI apply inference?" "Did AI calculate?" "Did AI acknowledge tension?"
- **Impact**: No enforcement even when facts present

**5. EARLY CLASSIFICATION SKIP (Affects CMP2, potentially others)**
- `orchestrator.js:883-915` - Early query classification
- Can skip memory retrieval entirely for "simple" queries
- **Impact**: Facts never reach AI

---

## CRITICAL FINDINGS SUMMARY

| Finding | Location | Impact | Tests Affected |
|---------|----------|--------|----------------|
| **F1**: Relationship keywords may be lost in extraction | intelligent-storage.js:1106-1268 | Implicit facts not preserved | INF1 |
| **F2**: No AI instruction to infer from implicit facts | orchestrator.js:3628 | AI doesn't apply inference | INF1 |
| **F3**: No inferencing validator | orchestrator.js:164 | No enforcement | INF1 |
| **F4**: Temporal anchors stored but not retrieved | orchestrator.js:2258 | Metadata not surfaced | INF3 |
| **F5**: Metadata not injected into AI context | orchestrator.js:2364 | Structured hints missing | INF3 |
| **F6**: No explicit temporal arithmetic instruction | orchestrator.js:3628 | AI doesn't calculate | INF3 |
| **F7**: No entity disambiguation at storage | intelligent-storage.js:818 | Multiple entities conflated | NUA1 |
| **F8**: Only 1 entity returned despite 2 stored | semantic-retrieval.js + orch:2291 | Incomplete data for ambiguity | NUA1 |
| **F9**: Hard cap prevents full entity set retrieval | orchestrator.js:2291 | Ambiguity undetectable | NUA1 |
| **F10**: Tension acknowledgment instruction is soft | orchestrator.js:4201 | No guarantee AI complies | NUA2 |
| **F11**: Memory not retrieved for CMP2 test | orchestrator.js:883-915 or semantic-retrieval.js | Zero data to work with | CMP2 |
| **F12**: Early classification may skip personal queries | orchestrator.js:900 | False negative | CMP2 |
| **F13**: Possible low similarity rank for name query | semantic-retrieval.js | Below TOP 5 cutoff | CMP2 |

---

## OTHER TESTS POTENTIALLY AFFECTED

Based on shared code paths, these test categories may have similar issues:

**INF2 (Role Inference - code review ‚Üí developer)**:
- Same path as INF1
- If test passes, extraction may handle job-related terms better than family terms

**STR1 (Volume - find car among 10 facts)**:
- Affected by MAX_MEMORIES_FINAL = 5 cap
- If car fact ranks #6-10, it won't be retrieved
- Test may be passing by luck (car happens to rank in TOP 5)

**STR2 (Discrimination - 3 Johns)**:
- Similar to NUA1 (multiple entities)
- If query specifies "my brother John", semantic search may successfully filter
- But general "John" query would face same ambiguity issues

**CMP1 (Name Preservation - Bartholomew)**:
- Same path as CMP2
- Single name easier to retrieve than multiple
- Extraction rule 2 preserves names

**ALL EDGE TESTS (EDG1-3)**:
- Share same retrieval and reasoning paths
- EDG3 (Numerical) uses same metadata extraction as INF3
- Pricing anchors (line 422) stored in metadata but likely not injected

---

## FILES INVOLVED (Complete List)

### Primary Flow:
1. **server.js** (lines 329-580) - Entry point, orchestrator handoff
2. **api/core/orchestrator.js** (lines 740-4300) - Complete processing flow
3. **api/services/semantic-retrieval.js** (lines 1-700) - Memory retrieval with semantic search
4. **api/memory/intelligent-storage.js** (lines 818-1268) - Extraction, fingerprinting, storage

### Supporting Systems:
5. **api/services/embedding-service.js** - Generates embeddings for semantic search
6. **api/core/intelligence/semantic_analyzer.js** - Intent/importance analysis
7. **api/categories/memory/index.js** - Memory system core & intelligence
8. **api/lib/validators/** (multiple files) - Enforcement validators

### Configuration:
9. **api/config/modes.js** - Mode definitions and compliance
10. **api/memory/pii-sanitizer.js** - PII sanitization before injection

---

## NEXT STEPS (DO NOT IMPLEMENT - INVESTIGATION ONLY)

### High Priority Fixes:

**For INF1 (Role Inference)**:
1. Add explicit relationship preservation to extraction prompt (intelligent-storage.js:1106)
2. Add reasoning instructions to system prompt requiring inference from implicit facts
3. Create inference validator to check "Was implicit information used?"

**For INF3 (Temporal Reasoning)**:
1. Surface metadata anchors during memory injection (orchestrator.js:2364)
2. Add structured hint format: `[TEMPORAL_ANCHOR: end=2020, duration=5, calculate=start_year]`
3. Add hard requirement in system prompt: "Calculate temporal values when anchors present"
4. Create arithmetic validator to check calculation compliance

**For NUA1 (Ambiguity)**:
1. Implement entity disambiguation at storage time (mark distinct entities)
2. Either: Increase MAX_MEMORIES_FINAL for entity queries
3. OR: Create secondary retrieval pass specifically for ambiguity detection (as comment suggests)
4. Enhance conflict validator to query DB directly for entity counts

**For NUA2 (Tension)**:
1. Create tension/conflict validator that checks:
   - Were opposing facts both present in context?
   - Did response mention both sides?
   - Did response acknowledge the tension/tradeoff?
2. Make tension acknowledgment a HARD requirement, not soft guidance

**For CMP2 (Name Preservation)**:
1. Debug why memory not retrieved: Add diagnostics to early classification
2. Ensure personal queries never skip retrieval
3. If similarity issue: Boost exact-match queries (contains "contacts")
4. Add character preservation validator for Unicode/diacritics

### Architectural Considerations:

**Metadata Injection Architecture** (Affects multiple tests):
- Current: Only `content` field injected
- Needed: Structured metadata hints alongside content
- Format: `[MEMORY_123] Content text [METADATA: {anchors, ordinals, entities}]`

**Validator Extension Framework**:
- Need validators that check REASONING application, not just content matching
- Validators need ability to:
  - Query database directly (for ambiguity counts)
  - Parse and validate calculations (for temporal)
  - Check inference chains (for implicit facts)

**Retrieval Strategy**:
- MAX_MEMORIES_FINAL=5 too restrictive for complex queries
- Need dynamic cap based on query type:
  - Ambiguity queries: retrieve ALL matching entities
  - Temporal queries: retrieve ALL temporal facts
  - Standard queries: current cap OK

---

## CONFIDENCE LEVELS

- **INF1 (Role Inference)**: 85% confidence in root cause
- **INF3 (Temporal Reasoning)**: 90% confidence in root cause
- **NUA1 (Ambiguity Detection)**: 95% confidence in root cause
- **NUA2 (Contextual Tension)**: 80% confidence in root cause
- **CMP2 (International Names)**: 85% confidence in root cause

All analyses are based on static code review. Runtime traces would increase confidence to 95%+.

---

## INVESTIGATION COMPLETE

This investigation has identified the complete data flow from input ‚Üí storage ‚Üí retrieval ‚Üí AI generation ‚Üí validation, and pinpointed specific failure points for each of the 5 tests. All findings are documented with file paths and line numbers.

No code changes have been made per the issue requirements. Ready for founder review and fix authorization.

---END SUMMARY---
