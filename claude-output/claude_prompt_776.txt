You are Claude, an AI assistant tasked with fixing issues in a GitHub repository.

Issue #776: [claude-fix] COMPREHENSIVE FIX: Injection Pipeline Restoration â€” All Confirmed Problems

Issue Description:
# COMPREHENSIVE FIX: Injection Pipeline Restoration â€” All Confirmed Problems

## Priority: CRITICAL
## Constraint: ZERO REGRESSION
## Based on: Dual AI audit (Claude Code #773 + Copilot independent verification) â€” both confirmed identical findings

---

## CRITICAL INSTRUCTIONS

This issue addresses ALL confirmed high-severity and medium-severity injection pipeline problems in ONE comprehensive fix. This is intentional â€” piecemeal fixes have caused build/break cycles. This fix must be complete and correct.

### Rules:
1. **Investigate BEFORE changing** â€” for each fix below, verify the file:line references are still accurate before editing
2. **Each fix is independent** â€” if one fix cannot be completed safely, skip it and note why. Do NOT let one failure block others.
3. **No refactoring** â€” only add or modify what is specified. Do not reorganize code, rename variables, or "improve" anything not listed.
4. **Test after EACH fix** â€” the system must remain deployable after each individual change
5. **Total changes should be under 150 lines across all files** â€” if significantly more is needed, stop and explain why

---

## FIX 1: Add `source_type` labeling to memory storage
### Severity: ðŸ”´ HIGH â€” Root cause of Problems #4 AND #8
### Files: `api/core/orchestrator.js` (memory storage section, ~line 1560-1750)

### Problem:
When the AI generates a response about a document or external data, that response is stored as a regular memory with no way to distinguish it from a normal user conversation. This causes:
- Old document analysis competing with new document uploads (Problem #4)
- Yesterday's Bitcoin price being treated as current data (Problem #8)

### What to do:
Find where `storeConversation()` or `storeWithSupersession()` is called (~line 1560-1750). At the point where the memory content is being prepared for storage, add a `source_type` tag to the content string itself.

**Do NOT alter the database schema.** Instead, prepend a tag to the stored content:

```javascript
// Before storing memory, tag the content with its source type
let contentToStore = aiResponse;

// If this response was about an uploaded document, tag it
if (context.sources?.hasDocuments) {
  contentToStore = `[SOURCE:document] ${contentToStore}`;
}

// If this response used external real-time data, tag it  
if (context.externalData?.success) {
  contentToStore = `[SOURCE:external_data:${new Date().toISOString()}] ${contentToStore}`;
}
```

This is non-destructive â€” existing memories are unaffected, and the tag is just a string prefix.

### Then: In memory retrieval, deprioritize stale tagged memories

Find where memories are scored/filtered in the orchestrator (near where `DIAG-NUA1` logs appear, the memory injection section).

When a NEW document is present (`context.sources?.hasDocuments === true`), filter OUT memories tagged with `[SOURCE:document]`:

```javascript
// If user has uploaded a new document, exclude old document memories
if (context.sources?.hasDocuments && memory.content?.startsWith('[SOURCE:document]')) {
  console.log(`[MEMORY-FILTER] Excluding old document memory ID:${memory.id} â€” new document present`);
  continue; // skip this memory
}
```

When external data was freshly fetched, filter OUT old external data memories:

```javascript
// If fresh external data available, exclude stale external data memories
if (context.externalData?.success && memory.content?.startsWith('[SOURCE:external_data:')) {
  console.log(`[MEMORY-FILTER] Excluding stale external data memory ID:${memory.id} â€” fresh data available`);
  continue;
}
```

### What NOT to do:
- Do NOT run ALTER TABLE or change the database schema
- Do NOT change how memories are retrieved from the database (the SQL query)
- Do NOT change the semantic similarity scoring
- Do NOT change cross-category safety checks or the safety boost logic
- Do NOT change how memories are displayed in the prompt template

---

## FIX 2: Fix document storage key from "latest" to unique ID
### Severity: ðŸŸ¡ MEDIUM â€” Problem #2
### File: `api/upload-for-analysis.js` (~line 534)

### Problem:
All uploaded documents are stored with the key `"latest"`, meaning each new upload overwrites the previous one in the backend Map.

### What to do:
Change the single line:
```javascript
// BEFORE (line 534):
extractedDocuments.set("latest", { ... });

// AFTER:
const documentKey = `doc_${Date.now()}_${file.originalname.replace(/[^a-zA-Z0-9]/g, '_')}`;
extractedDocuments.set(documentKey, { ... });
```

Also find where `extractedDocuments.get("latest")` is called in the orchestrator (`orchestrator.js:3356` per audit) and change it to get the MOST RECENT document:

```javascript
// BEFORE:
const latestDoc = extractedDocuments.get("latest");

// AFTER:
// Get the most recently added document from the Map
let latestDoc = null;
let latestTimestamp = 0;
for (const [key, doc] of extractedDocuments.entries()) {
  if (doc.timestamp > latestTimestamp) {
    latestTimestamp = doc.timestamp;
    latestDoc = doc;
  }
}
```

### What NOT to do:
- Do NOT change the extractedDocuments Map structure or max capacity
- Do NOT change the auto-cleanup timer (60 seconds, 10 minutes age)
- Do NOT change the multer configuration
- Do NOT change the frontend upload handler

---

## FIX 3: Add commodity fallback to Google News RSS
### Severity: ðŸŸ¡ MEDIUM â€” Problems #6 and #7
### File: `api/core/intelligence/externalLookupEngine.js`

### Problem:
Precious metals queries (gold, silver) are routed ONLY to Metals-API (401) and Goldapi.io (403). When both fail, the system returns graceful degradation instead of trying Google News RSS, which is free, working, and can provide commodity pricing from news articles.

### What to do:
Find the `selectSourcesForQuery` function. Find where COMMODITIES type queries are routed. Add Google News RSS as a FALLBACK source for commodity queries:

```javascript
// In the source selection for COMMODITIES queries, add news RSS as fallback
// After the existing COMMODITIES sources, add:
if (queryType === 'COMMODITIES' || query.match(/\b(gold|silver|platinum|palladium|copper|oil|commodity|commodities)\b/i)) {
  // Try dedicated commodity APIs first (if keys are set)
  sources.push(...commoditySources);
  
  // FALLBACK: Also try Google News RSS â€” commodity prices are news topics
  sources.push({
    name: 'Google News RSS (commodity fallback)',
    url: () => `https://news.google.com/rss/search?q=${encodeURIComponent(query + ' price today')}&hl=en-US&gl=US&ceid=US:en`,
    parser: 'rss',
    type: 'news_fallback'
  });
}
```

**Key principle**: Don't remove the commodity API sources â€” just add the news fallback AFTER them. If someone later sets real API keys, the dedicated sources will work and the news fallback won't be needed.

### Also: Remove hardcoded fake API keys
Find lines ~90 and ~115 where `'FREE'` and `'goldapi-demo-key'` are used as fallback values. Change them so if the env var isn't set, the source is SKIPPED entirely rather than making a doomed API call:

```javascript
// BEFORE:
const apiKey = process.env.METALS_API_KEY || 'FREE';

// AFTER:
const apiKey = process.env.METALS_API_KEY;
if (!apiKey) {
  console.log('[externalLookupEngine] METALS_API_KEY not set, skipping Metals-API source');
  // Don't add this source â€” let it fall through to news fallback
} else {
  sources.push(metalsApiSource);
}
```

Same pattern for `GOLDAPI_KEY`.

### What NOT to do:
- Do NOT remove the Metals-API or Goldapi.io source definitions (they work if real keys are provided later)
- Do NOT change CoinGecko, Wikipedia, Exchange Rates, or Google News RSS configurations
- Do NOT change the truth type detector or hierarchy router
- Do NOT change the graceful degradation protocol
- Do NOT change the TTL cache manager

---

## FIX 4: Add external data to token budget enforcement
### Severity: ðŸ”´ HIGH â€” Problem #11
### File: `api/core/orchestrator.js` (~line 3700-3720 per audit, near `#routeToAI`)

### Problem:
Token budget calculation accounts for system prompt, memory, vault, and documents â€” but NOT external data. External data can be up to 15,000 chars (~3,750 tokens). Combined worst case: 8,825 tokens vs 8,192 model limit = 633 tokens over.

### What to do:
Find where external data is added to the context (in `#buildContextString`, near line 4162-4203 per audit). Add truncation for external data similar to how PR #771 handles document truncation:

```javascript
// BEFORE external data is injected into prompt string:
const MAX_EXTERNAL_CHARS = 4000; // ~1000 tokens, leaving room for other sections
if (externalDataString && externalDataString.length > MAX_EXTERNAL_CHARS) {
  const originalLength = externalDataString.length;
  externalDataString = externalDataString.substring(0, MAX_EXTERNAL_CHARS) +
    '\n\n[External data truncated from ' + originalLength + ' characters to fit token budget.]';
  console.log(`[EXTERNAL-TRUNCATE] Truncated external data from ${originalLength} to ${MAX_EXTERNAL_CHARS} chars`);
}
```

Find the variable name used for external data in `#buildContextString` â€” it may be `externalContext`, `externalDataStr`, or similar. Apply truncation to THAT variable, following the same pattern as the document truncation at line 4354-4362.

### What NOT to do:
- Do NOT change the document truncation (PR #771, working)
- Do NOT change memory truncation or retrieval limits
- Do NOT change the model selection or API call configuration
- Do NOT change the vault content loading

---

## FIX 5: Allow document injection alongside vault in Site Monkeys mode
### Severity: ðŸŸ¡ MEDIUM â€” Problem #10
### File: `api/core/orchestrator.js` (~line 4243)

### Problem:
In Site Monkeys mode, when vault content is present, an early `return contextStr;` at line 4243 prevents document content from being injected. Users cannot analyze uploaded documents while in Site Monkeys mode.

### What to do:
Remove the early return. Instead, inject BOTH vault and document content, with vault taking priority for business questions:

```javascript
// BEFORE (line 4243):
return contextStr;  // <-- This prevents documents from being injected

// AFTER:
// Don't return early â€” continue to document injection section below
// Vault content is already in contextStr, documents will be added after
```

If the early return is needed for a different reason, instead add a conditional:

```javascript
// Only skip document injection if there's no document uploaded
if (!context.sources?.hasDocuments) {
  return contextStr;
}
// Otherwise, fall through to add document content alongside vault
```

### What NOT to do:
- Do NOT change how vault content is loaded or formatted
- Do NOT change vault priority for business policy claims
- Do NOT change the vault template or section headers
- Do NOT change mode routing logic

---

## WHAT MUST NOT BE CHANGED (across all fixes)

These systems are WORKING correctly per both audits. Do NOT touch them:

- âœ… Memory retrieval semantic scoring (cosine similarity)
- âœ… Cross-category safety checks (allergy checking on restaurant queries)
- âœ… Safety boost (+0.25 for health/allergy memories)
- âœ… Early classification optimization (skip memory for greetings)
- âœ… Numerical data extraction
- âœ… Truth type detector (Stage 1 deterministic rules)
- âœ… Hierarchy router (business policy vs objective factual)
- âœ… TTL cache manager
- âœ… CoinGecko, Google News RSS, Wikipedia, Exchange Rates API configurations
- âœ… Graceful degradation three-step protocol (DISCLOSE â†’ PROVIDE â†’ PATH)
- âœ… Frontend upload handler and hourglass button
- âœ… Multer configuration (PR #769)
- âœ… Document truncation (PR #771)
- âœ… Unicode/compression handling (PR #759, #761)
- âœ… Test detection logic
- âœ… Mode routing and personality switching
- âœ… Prompt section headers and template formatting
- âœ… Cross-mode transfer logic

---

## VALIDATION AFTER ALL FIXES

### Document pipeline:
1. Upload Document A â†’ AI analyzes Document A âœ…
2. Upload Document B â†’ AI analyzes Document B (NOT A) âœ…
3. Upload document in Site Monkeys mode â†’ AI analyzes document (vault still present for business questions) âœ…
4. Upload large document â†’ truncated with notice, no crash âœ…

### External data pipeline:
5. "What's the price of Bitcoin?" â†’ CoinGecko returns data, AI gives confident answer âœ…
6. "What's the price of gold?" â†’ Falls back to Google News RSS, AI provides best available info âœ…
7. "What's in the news about Trump?" â†’ Google News RSS returns data âœ…
8. No `context_length_exceeded` errors even with external data + memory + documents âœ…

### Memory pipeline:
9. Upload new document â†’ old document memories NOT injected alongside new content âœ…
10. Fresh external data available â†’ stale external data memories NOT injected âœ…
11. Normal conversation memories â†’ work exactly as before âœ…
12. Cross-category safety checks â†’ still fire for restaurant/allergy queries âœ…

### Test suites:
13. 53-innovation suite â†’ maintain 60/61 (98%) âœ…
14. SMDEEP â†’ maintain 12-14/15 (80-93%) âœ…

---

## IMPLEMENTATION ORDER

1. Fix 4 first (token budget) â€” prevents crashes, lowest risk
2. Fix 3 second (commodity fallback) â€” adds capability, no existing code modified
3. Fix 1 third (source_type tagging) â€” fixes the two high-severity memory problems
4. Fix 2 fourth (document key) â€” prevents document overwrites
5. Fix 5 last (vault + document) â€” changes control flow, highest risk

If any fix cannot be completed safely, SKIP IT and document why. The other fixes should still be applied independently.
Your task is to:
1. Analyze the issue carefully to understand the problem
2. Look through the repository to identify the relevant files that need to be modified
3. Make precise changes to fix the issue
4. Use the Edit tool to modify files directly when needed
5. Be minimal in your changes - only modify what's necessary to fix the issue

After making changes, provide a summary of what you did in this format:

---SUMMARY---
[Your detailed summary of changes, including which files were modified and how]
---END SUMMARY---

Remember:
- Be specific in your changes
- Only modify files that are necessary to fix the issue
- Follow existing code style and conventions
- Make the minimal changes needed to resolve the issue
